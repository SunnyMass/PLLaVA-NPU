<div align="center">

<h2><a href="https://pllava.github.io/">PLLaVA-NPU: Adapt PLLaVA program to run on NPU</a></h2>

Tiange Zhang, Rongqun Lin, Chuanmin Jia, Siwei Ma

Peking University, Pengcheng Laboratory

</div>

<!-- [![Paper](https://img.shields.io/badge/cs.CV-2311.17005-b31b1b?logo=arxiv&logoColor=red)](https://arxiv.org/abs/2311.17005) -->

**Project Page: [PLLaVA-NPU](https://github.com/SunnyMass/PLLaVA-NPU.git)**



### å·¥ä½œç®€è¿°
æœ¬é¡¹ç›®æ—¨åœ¨å°†å¤šæ¨¡æ€è§†é¢‘ç†è§£æ¨¡å‹ **PLLaVA** é€‚é…è‡³åä¸ºæ˜‡è…¾910B èŠ¯ç‰‡å¹³å°ï¼Œå®ç°å…¶åœ¨ NPU ä¸Šçš„å®Œæ•´æ¨ç†æµç¨‹ä¸éƒ¨ç½²æ”¯æŒã€‚PLLaVA ä¸ºé€šç”¨å‹å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œæ”¯æŒå¤šç§è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œå¦‚è§†é¢‘é—®ç­”ï¼ˆVideoQAï¼‰ä¸è§†é¢‘å­—å¹•ç”Ÿæˆï¼ˆVideo Captioningï¼‰ï¼Œå…¶æ ¸å¿ƒä¾èµ–åŒ…æ‹¬ 3D æ± åŒ–ç®—å­å’Œ `decord` è§†é¢‘è¯»å–æ¨¡å—ï¼Œå‡æ— æ³•åœ¨æ˜‡è…¾ NPU ä¸Šç›´æ¥è¿è¡Œï¼Œéœ€è¿›è¡Œåº•å±‚æ¨¡å—é‡æ„ä¸å…¼å®¹æ€§é€‚é…ã€‚

æœ¬é¡¹ç›®åŸºäº PLLaVA å®˜æ–¹å¼€æºä»£ç å®Œæˆé€‚é…ä¸ä¼˜åŒ–ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

- âœ… é‡å†™ 3D æ± åŒ–æ¨¡å—ç­‰è„šæœ¬ä»¥é€‚é… NPU è¿ç®—ï¼›
- âœ… ä½¿ç”¨ `cv2` æ›¿ä»£ `decord`ï¼Œå®Œæˆè§†é¢‘å¸§çš„è¯»å–ä¸é¢„å¤„ç†ï¼›
- âœ… æ”¯æŒ Gradio ç½‘é¡µç«¯éƒ¨ç½²åŠå‘½ä»¤è¡Œè°ƒç”¨æ–¹å¼ï¼›
- âœ… ä¸ºè§†é¢‘captionä»»åŠ¡æä¾›äº†æ›´ä¸°å¯Œçš„èŠå¤©èƒŒæ™¯è®¾å®šï¼Œç»™å®šç¤ºä¾‹æŒ‡å¯¼ç”Ÿæˆæ›´ä¼˜ç»“æœã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹æ¯”äº†é€‰æ‹© 4 å¸§ã€16 å¸§ç­‰è¾“å…¥é…ç½®ã€‚ç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ 16 å¸§æ–¹æ¡ˆåœ¨è¯­ä¹‰è´¨é‡ä¸å“åº”æ—¶é—´ä¹‹é—´è¾¾åˆ°è¾ƒå¥½å¹³è¡¡ã€‚æœ€åï¼Œåœ¨ GPU å¹³å°è¿›è¡Œå¯¹ç…§æµ‹è¯•ï¼Œç»“æœæ˜¾ç¤ºè¯­ä¹‰è¾“å‡ºä¸€è‡´ï¼Œå…¶ä¸­3Dæ± åŒ–æ¨¡å—çš„è¾“å…¥å’Œè¾“å‡ºå‡ä¿æŒä¸€è‡´ã€‚

ç›®å‰é¡¹ç›®ä»£ç å·²æ•´ç†å®Œæˆå¹¶å¼€æºï¼Œæ”¯æŒ NPU å¹³å°è¿è¡Œ PLLaVA é€šç”¨ chat æ¨¡å‹çš„è§†é¢‘ç†è§£ä»»åŠ¡ã€‚æ¬¢è¿åœ¨æ­¤åŸºç¡€ä¸Šç»§ç»­ä¼˜åŒ–è¿è¡Œæ•ˆç‡ï¼Œæ‹“å±•æ›´å¤šæ¨¡æ€åº”ç”¨åœºæ™¯ã€‚

### å®éªŒç»“æœ
**æµ‹è¯•è§†é¢‘è¾“å…¥ï¼š**

![æµ‹è¯•è§†é¢‘](./1-2.mp4)

**è¿è¡Œå‘½ä»¤ï¼š**
```
python run_demo.py   --video_path path_to_1-2.mp4   --prompt "describe this video in detail"   --pretrained_model_name_or_path path_to_pllava7b   --weight_dir path_to_pllava7b   --use_lora   --num_frames 16   --conv_mode plain   --max_new_tokens 128  --video_caption(å¦‚æœæ˜¯åšè§†é¢‘captionä»»åŠ¡å°±åŠ ä¸Šï¼Œå¦‚æœæ˜¯å…¶ä»–è§†é¢‘ç†è§£ä»»åŠ¡å°±ä¸åŠ )
```
å¦‚æœéœ€è¦éƒ¨ç½²gradio
```
sh ./scripts/demo.sh
```
å¯ä»¥åœ¨PLLaVA/tasks/eval/demo/pllava_demo.pyæ–‡ä»¶æœ«å°¾å®šä¹‰urlåœ°å€ã€‚
```
demo.launch(
    server_name="0.0.0.0",
    server_port=10034,
    root_path="/ai/api/proxy/ascend-k8s/relative/master/30003/proxy/10034"
)
```
![NPU æ¨ç†ç»“æœ](./assert/gradio.png)
**NPUè¾“å‡ºç»“æœï¼šï¼ˆé‡å†™3Dæ± åŒ–ç®—å­ï¼‰**
```
The image depicts a group of men in a wooded area, with some of them carrying guns. They are walking through the forest, possibly engaging in a hunting activity or a military exercise. The men are spread out across the scene, with some closer to the foreground and others further back. The presence of the guns suggests that they are prepared for a potential threat or are participating in a training exercise. The overall atmosphere of the image is one of camaraderie and readiness.
```

**GPUè¾“å‡ºç»“æœï¼šï¼ˆæ”¯æŒAdaptiveAvgPool3dç®—å­ï¼‰**
```
The image depicts a group of men in a wooded area, with some of them carrying guns. They are walking through the forest, possibly on a mission or engaging in a training exercise. The men are spread out, with some closer to the front and others further back in the scene. The presence of the guns suggests that they might be prepared for a potential threat or are participating in a military or law enforcement operation.
```
### é‡å†™3Dæ± åŒ–ç®—å­
**åŸå§‹ä»£ç é—®é¢˜åˆ†æï¼š**
RuntimeError: adaptive_avg_pool3d only support D=1 && H=1 && W=1 current!è¯¥é—®é¢˜æŒ‡å‡ºï¼ŒNPUä¸æ”¯æŒ3Dæ± åŒ–ï¼Œå³ä¸æ”¯æŒtorch._C._nn.adaptive_avg_pool3dè‡ªé€‚åº”å°ºå¯¸ã€‚å¦‚æœè§£å†³è¿™ç§æŠ¥é”™ï¼Œå¿…é¡»è¦ä¿®æ”¹å°ºå¯¸ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹æ¨ç†æ—¶ï¼Œå‡ºç°é”™è¯¯çš„æ˜ å°„å¹¶æ— æ³•å¯¹é½ï¼Œä»è€Œå¯¼è‡´å¹»è§‰ç­‰é—®é¢˜å‡ºç°ã€‚å¦‚æœæ”¹æˆ2Dæ± åŒ–æˆ–è€…ç”¨å…¶ä»–æ–¹å¼è¡¨å¾æ—¶åŸŸï¼Œéƒ½æ— æ³•æ”¹å–„è¿™ç§å¹»è§‰é—®é¢˜ï¼Œè€Œåœ¨GPUä¸Šæ¨ç†èƒ½å¾—åˆ°å‡†ç¡®ç»“æœã€‚å› æ­¤éœ€è¦ä»æœ¬è´¨ä¸Šè§£å†³è¿™ä¸ª3Dæ± åŒ–å‡½æ•°é€‚é…é—®é¢˜ã€‚
AdaptiveAvgPool3d((T_out, H_out, W_out)) è¡¨ç¤ºï¼šåœ¨ä¸‰ç»´ç©ºé—´ï¼ˆæ—¶é—´ + ç©ºé—´ï¼‰ä¸Šï¼Œè‡ªåŠ¨å°†æ¯æ®µ videoï¼ˆå½¢çŠ¶ [C, T, H, W]ï¼‰å¹³å‡æ± åŒ–åˆ°æŒ‡å®šå½¢çŠ¶ [C, T_out, H_out, W_out]ã€‚å¦‚æœè¾“å…¥æ˜¯ [B, C, T=8, H=14, W=14]ï¼Œè®¾ç½® --pooling_shape 4-12-12ï¼Œå°±ä¼šå˜æˆï¼šè¾“å‡ºå½¢çŠ¶ = [B, C, 4, 12, 12]ï¼Œä»è€Œç¨€ç–åŒ–tokenä¸ªæ•°ã€‚

**ä»£ç æµç¨‹ï¼š**
çº¿æ€§æ˜ å°„ vision hidden â†’ text hiddenï¼›
è½¬æ¢ä¸º [B, C, T, H, W]ï¼›
ç”¨ 3Dæ± åŒ–ï¼ˆæ—¶é—´+ç©ºé—´ï¼‰é™ä½ token æ•°ï¼›
flatten æˆ [B, N, dim] ä¾› LLM æ¥å—ï¼›
æœ€ç»ˆç”¨äº generate() æ¥å£è¾“å…¥çš„ image_featuresã€‚

**æ›¿ä»£è§£å†³æ–¹æ¡ˆï¼š**
ç”±äº NPU ä¸æ”¯æŒåŒ…æ‹¬ AdaptiveAvgPool3d åœ¨å†…çš„å¤šç§ 3D æ± åŒ–ç®—å­ï¼Œå› æ­¤æˆ‘ä»¬æ‰‹åŠ¨å®ç°ä¸€ä¸ªå…¼å®¹ NPU çš„ 3D æ± åŒ–å‡½æ•°ï¼Œç”¨äºåœ¨ä¸æ”¹å˜æ•°å€¼è¯­ä¹‰çš„å‰æä¸‹ï¼Œå®Œæˆæ—¶é—´ç»´åº¦å’Œç©ºé—´ç»´åº¦çš„é™é‡‡æ ·ï¼Œç¡®ä¿æ¨ç†è¿‡ç¨‹ä¸ GPU ä¸Šä¸€è‡´ï¼Œé¿å…å‡ºç°æ˜ å°„é”™ä¹±æˆ–è¯­ä¹‰å¹»è§‰ç­‰é—®é¢˜ã€‚è¯¥å‡½æ•°å®Œå…¨å¤ç°äº† AdaptiveAvgPool3d çš„è¡Œä¸ºï¼Œå‰ææ˜¯è¾“å…¥çš„æ—¶é—´ã€ç©ºé—´å°ºå¯¸èƒ½æ•´é™¤ç›®æ ‡å°ºå¯¸ã€‚é€‚ç”¨äºå¤§å¤šæ•°å®é™…é…ç½®ï¼ŒåŒæ—¶åœ¨ NPU ä¸Šé«˜æ•ˆå¯ç”¨ï¼Œç¡®ä¿è§†è§‰ç‰¹å¾ token æ•°åœ¨æ—¶ç©ºç»´åº¦çš„ç¨€ç–åŒ–è¿‡ç¨‹ä¿æŒä¸€è‡´ã€‚


```
def adaptive_avg_pool3d_manual(self, x, output_size):
        """
        x: [B, C, D, H, W]
        output_size: (d_out, h_out, w_out)
        æ›¿ä»£ AdaptiveAvgPool3dï¼ŒNPU å…¼å®¹
        """
        B, C, D, H, W = x.shape
        d_out, h_out, w_out = output_size
        print(output_size)
        assert D % d_out == 0 and H % h_out == 0 and W % w_out == 0, "Input size must be divisible by output size"

        kd = D // d_out
        kh = H // h_out
        kw = W // w_out

        # reshape æˆ 6ç»´ï¼šå°† D/H/W åˆ†æˆ avg block å— + å—å†…å…ƒç´ 
        x = x.view(B, C, d_out, kd, h_out, kh, w_out, kw)  # [B, C, d_out, kd, h_out, kh, w_out, kw]
        x = x.mean(dim=(3, 5, 7))  # å¯¹ kd, kh, kw ä¸‰ä¸ªç»´åº¦åšå‡å€¼
        return x  # shape [B, C, d_out, h_out, w_out]
```

### è¿è¡Œè„šæœ¬
```
import torch
import time
import cv2
from PIL import Image
from argparse import ArgumentParser
import torchvision.transforms as transforms

from tasks.eval.eval_utils import conv_templates, ChatPllava
from tasks.eval.model_utils import load_pllava

SYSTEM = """You are a powerful Video Magic ChatBot, a large vision-language assistant. 
You are able to understand the video content that the user provides and assist the user in a video-language related task.
The user might provide you with the video and maybe some extra noisy information to help you out or ask you a question. Make use of the information in a proper way to be competent for the job.
### INSTRUCTIONS:
1. Follow the user's instruction.
2. Be critical yet believe in yourself.
"""
SYSTEM2 = """
Describe this video. Pay attention to all objects in the video. The description should be useful for AI to re-generate the video. The description should be no more than six sentences. Here are some examples of good descriptions: 1. A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about. 2. Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field. 3. Drone view of waves crashing against the rugged cliffs along Big Sur's garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff's edge. The steep drop from the road down to the beach is a dramatic feat, with the cliffâ€™s edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.
"""

def load_video(video_path, num_segments=4, resolution=336):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    indices = [int(i * total_frames / num_segments) for i in range(num_segments)]

    frames = []
    resize = transforms.Resize((resolution, resolution))
    idx = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if idx in indices:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_img = Image.fromarray(frame)
            pil_img = resize(pil_img)
            frames.append(pil_img)
        idx += 1
    cap.release()
    return frames


def parse_args():
    parser = ArgumentParser()
    parser.add_argument('--video_path', type=str, required=True)
    parser.add_argument('--prompt', type=str, required=True)
    parser.add_argument('--num_frames', type=int, default=4)
    parser.add_argument('--pretrained_model_name_or_path', type=str, required=True)
    parser.add_argument('--weight_dir', type=str, default=None)
    parser.add_argument('--use_lora', action='store_true')
    parser.add_argument('--lora_alpha', type=int, default=4)
    parser.add_argument('--conv_mode', type=str, default='plain')
    parser.add_argument('--max_new_tokens', type=int, default=200)
    parser.add_argument('--num_beams', type=int, default=1)
    parser.add_argument('--temperature', type=float, default=1.0)
    parser.add_argument('--video_caption', action='store_true')
    return parser.parse_args()


def main():
    args = parse_args()

    print("ğŸ“¦ Loading model...")
    model, processor = load_pllava(
        repo_id=args.pretrained_model_name_or_path,
        num_frames=args.num_frames,
        use_lora=args.use_lora,
        lora_alpha=args.lora_alpha,
        weight_dir=args.weight_dir,
    )
    model = model.to('npu').eval()
    print(f"Model device: {next(model.parameters()).device}")
    chat = ChatPllava(model, processor)

    print("ğŸ“½ï¸ Loading video frames...")
    frames = load_video(args.video_path, args.num_frames)
    img_list = [frames]  # å¿…é¡»æ˜¯äºŒç»´åˆ—è¡¨ [ [PIL, PIL, PIL...] ]

    print("ğŸ’¬ Asking and answering...")
    conv = conv_templates[args.conv_mode].copy()
    if args.video_caption:
        conv = chat.ask(args.prompt, conv, SYSTEM2)
    else:
        conv = chat.ask(args.prompt, conv, SYSTEM)

    start_time = time.time()
    llm_message, _, _ = chat.answer(
        conv=conv,
        img_list=img_list,
        max_new_tokens=args.max_new_tokens,
        num_beams=args.num_beams,
        temperature=args.temperature
    )
    elapsed = time.time() - start_time

    print(f"\nâ±ï¸ Inference took {elapsed:.2f} seconds")
    print("\n===== FINAL ANSWER =====\n")
    print(llm_message.strip())


if __name__ == '__main__':
    main()

```

### ç¯å¢ƒé…ç½®
å½“å‰condaç¯å¢ƒé…ç½®ä¿å­˜åœ¨conda_env_export.txtæ–‡ä»¶ä¸­ã€‚
```
name: caption
channels:
  - defaults
  - https://repo.anaconda.com/pkgs/main
  - https://repo.anaconda.com/pkgs/r
dependencies:
  - _libgcc_mutex=0.1=main
  - _openmp_mutex=5.1=51_gnu
  - ca-certificates=2024.11.26=hd43f75c_0
  - ld_impl_linux-aarch64=2.40=h48e3ba3_0
  - libffi=3.4.4=h419075a_1
  - libgcc-ng=11.2.0=h1234567_1
  - libgomp=11.2.0=h1234567_1
  - libstdcxx-ng=11.2.0=h1234567_1
  - ncurses=6.4=h419075a_0
  - openssl=3.0.15=h998d150_0
  - pip=24.2=py39hd43f75c_0
  - python=3.9.20=h4bb2201_1
  - readline=8.2=h998d150_0
  - setuptools=75.1.0=py39hd43f75c_0
  - sqlite=3.45.3=h998d150_0
  - tk=8.6.14=h987d8db_0
  - wheel=0.44.0=py39hd43f75c_0
  - xz=5.4.6=h998d150_1
  - zlib=1.2.13=h998d150_1
  - pip:
      - accelerate==1.6.0
      - aiofiles==23.2.1
      - albucore==0.0.23
      - albumentations==1.4.24
      - annotated-types==0.7.0
      - antlr4-python3-runtime==4.9.3
      - anyio==4.9.0
      - asttokens==3.0.0
      - astunparse==1.6.3
      - attrs==24.2.0
      - av==14.0.1
      - beautifulsoup4==4.12.3
      - bs4==0.0.2
      - certifi==2024.8.30
      - charset-normalizer==3.4.0
      - click==8.1.8
      - contourpy==1.3.0
      - cycler==0.12.1
      - decorator==5.1.1
      - distro==1.9.0
      - einops==0.8.0
      - eval-type-backport==0.2.2
      - exceptiongroup==1.2.2
      - fastapi==0.115.12
      - ffmpy==0.5.0
      - filelock==3.16.1
      - fonttools==4.57.0
      - fsspec==2024.10.0
      - ftfy==6.3.1
      - gradio==4.44.1
      - gradio-client==1.3.0
      - h11==0.14.0
      - httpcore==1.0.7
      - httpx==0.28.1
      - huggingface-hub==0.30.1
      - idna==3.10
      - imageio==2.36.1
      - imageio-ffmpeg==0.6.0
      - imagesize==1.4.1
      - importlib-resources==6.5.2
      - jinja2==3.1.4
      - jiter==0.9.0
      - kiwisolver==1.4.7
      - lazy-loader==0.4
      - markdown-it-py==3.0.0
      - markupsafe==2.1.5
      - matplotlib==3.9.4
      - mdurl==0.1.2
      - mindcv==0.3.0
      - mindspore==2.3.1
      - moviepy==2.1.2
      - mpmath==1.3.0
      - networkx==3.2.1
      - numpy==1.26.4
      - omegaconf==2.3.0
      - openai==1.72.0
      - opencv-python==4.10.0.84
      - opencv-python-headless==4.10.0.84
      - orjson==3.10.16
      - packaging==24.2
      - pandas==2.2.3
      - peft==0.15.1
      - pillow==10.4.0
      - proglog==0.1.11
      - protobuf==5.29.2
      - psutil==6.0.0
      - pydantic==2.10.4
      - pydantic-core==2.27.2
      - pydub==0.25.1
      - pygments==2.19.1
      - pyparsing==3.2.3
      - python-dateutil==2.9.0.post0
      - python-dotenv==1.1.0
      - python-multipart==0.0.20
      - pytz==2024.2
      - pyvideoreader==0.5.7
      - pyyaml==6.0.2
      - regex==2024.11.6
      - requests==2.32.3
      - rich==14.0.0
      - ruff==0.11.4
      - safetensors==0.4.5
      - scikit-image==0.24.0
      - scipy==1.13.1
      - semantic-version==2.10.0
      - sentencepiece==0.2.0
      - shellingham==1.5.4
      - simsimd==6.2.1
      - six==1.17.0
      - sniffio==1.3.1
      - soupsieve==2.6
      - starlette==0.46.1
      - stringzilla==3.11.3
      - svgwrite==1.4.3
      - sympy==1.13.1
      - tifffile==2024.8.30
      - tokenizers==0.21.1
      - tomlkit==0.12.0
      - toolz==1.0.0
      - torch==2.3.1
      - torch-npu==2.3.1
      - torchaudio==0.13.1
      - torchvision==0.14.1
      - tqdm==4.67.1
      - transformers==4.51.0
      - typer==0.15.2
      - typing-extensions==4.12.2
      - tzdata==2024.2
      - urllib3==2.2.3
      - uvicorn==0.34.0
      - wcwidth==0.2.13
      - websockets==12.0
      - zipp==3.21.0
prefix: /root/software/miniconda3/envs/caption

```

### é¡¹ç›®ç»“æ„
```
.
â”œâ”€â”€ 1-2.mp4
â”œâ”€â”€ assert
â”‚Â Â  â”œâ”€â”€ data.png
â”‚Â Â  â”œâ”€â”€ gradio.png
â”‚Â Â  â”œâ”€â”€ logo.png
â”‚Â Â  â”œâ”€â”€ module.png
â”‚Â Â  â”œâ”€â”€ performance.png
â”‚Â Â  â”œâ”€â”€ teaser.jpg
â”‚Â Â  â””â”€â”€ zeroshot.png
â”œâ”€â”€ conda_env_export.txt
â”œâ”€â”€ DATA.md
â”œâ”€â”€ dataset
â”‚Â Â  â”œâ”€â”€ base_dataset.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ it_dataset.py
â”‚Â Â  â”œâ”€â”€ utils.py
â”‚Â Â  â””â”€â”€ video_utils.py
â”œâ”€â”€ docs
â”‚Â Â  â””â”€â”€ PoolLLaVA_Report.pdf
â”œâ”€â”€ example
â”‚Â Â  â”œâ”€â”€ 1917.mov
â”‚Â Â  â”œâ”€â”€ 1917.mp4
â”‚Â Â  â”œâ”€â”€ bear.jpg
â”‚Â Â  â”œâ”€â”€ cooking.mp4
â”‚Â Â  â”œâ”€â”€ dog.png
â”‚Â Â  â”œâ”€â”€ jesse_dance.mp4
â”‚Â Â  â”œâ”€â”€ working.mp4
â”‚Â Â  â””â”€â”€ yoga.mp4
â”œâ”€â”€ fusion_result.json
â”œâ”€â”€ models
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ pllava
â”‚Â Â  â””â”€â”€ __pycache__
â”œâ”€â”€ python_scripts
â”‚Â Â  â””â”€â”€ hf.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.no_torch.txt
â”œâ”€â”€ requirements.torch.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_demo.py
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ accel_config_deepspeed_zero2.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_deepspeed_zero3_offload_multinode_1.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_deepspeed_zero3_offload_multinode_2.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_deepspeed_zero3_offload_multinode.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_deepspeed_zero3_offload_singlegpu.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_deepspeed_zero3_offload.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_multigpu.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_multinode.yaml
â”‚Â Â  â”œâ”€â”€ accel_config_singlegpu.yaml
â”‚Â Â  â”œâ”€â”€ demo.sh
â”‚Â Â  â”œâ”€â”€ eval.sh
â”‚Â Â  â”œâ”€â”€ eval_yiprompt.sh
â”‚Â Â  â”œâ”€â”€ gallery.sh
â”‚Â Â  â”œâ”€â”€ train_pllava_13b.sh
â”‚Â Â  â”œâ”€â”€ train_pllava_34b.sh
â”‚Â Â  â”œâ”€â”€ train_pllava_7b.sh
â”‚Â Â  â””â”€â”€ train_pllava.sh
â”œâ”€â”€ structure.txt
â”œâ”€â”€ tasks
â”‚Â Â  â”œâ”€â”€ eval
â”‚Â Â  â”œâ”€â”€ shared_utils.py
â”‚Â Â  â””â”€â”€ train
â””â”€â”€ utils
    â”œâ”€â”€ basic_utils.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ config_utils.py
    â”œâ”€â”€ distributed.py
    â”œâ”€â”€ easydict.py
    â”œâ”€â”€ logger.py
    â”œâ”€â”€ optimizer.py
    â”œâ”€â”€ __pycache__
    â””â”€â”€ scheduler.py

15 directories, 59 files

```
